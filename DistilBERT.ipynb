{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HW9OPTn1nwq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "4r2pnZIC1pah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import shutil\n",
        "import sys\n",
        "import pickle\n",
        "import ast\n",
        "\n",
        "from torch import cuda\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "1pIhfRla1xDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Setting up the device for GPU usage\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "ukhrrDww9qv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('drive/MyDrive/NLP_Final_Project/dataset/final_classification.csv', delimiter=',')\n",
        "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "dataset['tags_list'] = dataset['tags_list'].apply(ast.literal_eval)\n",
        "tags_to_keep = ['love', 'life', 'inspirational', 'philosophy', 'humor']\n",
        "\n",
        "for tag in tags_to_keep:\n",
        "    dataset[tag] = dataset['tags_list'].apply(lambda x: 1 if tag in x else 0)\n",
        "\n",
        "print('Shape of dataset : ',dataset.shape)\n",
        "dataset.head(5)"
      ],
      "metadata": {
        "id": "NThd0WVj3oNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, random_seed):\n",
        "\n",
        "  train_val, test_df = train_test_split(df, test_size=0.2, random_state=random_seed, stratify=df['tags'])\n",
        "  train_df, val_df = train_test_split(train_val, test_size=0.15, random_state=random_seed, stratify=train_val['tags'])\n",
        "\n",
        "  for d in [train_df, val_df, test_df]:\n",
        "    tag_count = {}\n",
        "    for tag in d['tags_list']:\n",
        "        for t in tag:\n",
        "            tag_count[t] = tag_count.get(t, 0) + 1\n",
        "    print('Tag Distribution: ', tag_count)\n",
        "\n",
        "  train_df = train_df.reset_index()\n",
        "  val_df = val_df.reset_index()\n",
        "  test_df = test_df.reset_index()\n",
        "\n",
        "  return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "Wlx-xuGB6Wun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, valid_df, test_df = split_data(dataset, 204)"
      ],
      "metadata": {
        "id": "bN2F05Ei9fq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# hyperparameters\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 2e-05\n",
        "\n",
        "target_labels = tags_to_keep"
      ],
      "metadata": {
        "id": "DFSjlLEnCLug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.quote = dataframe.quote\n",
        "        self.targets = self.data[tags_to_keep].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.quote)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        quote = str(self.quote[index])\n",
        "        quote = \" \".join(quote.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            quote,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index]),\n",
        "            'quote' : quote\n",
        "        }"
      ],
      "metadata": {
        "id": "q1EPTmnZ5Cjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "# valid_dataset = CustomDataset(valid_df, tokenizer, MAX_LEN)\n",
        "# test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "1wLlq_F85evE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistilBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.classifier = torch.nn.Linear(768, 5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "metadata": {
        "id": "EH4k7vhB8UAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ],
      "metadata": {
        "id": "RfnxzA_Z8s4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = DistilBERTClass()\n",
        "# model.to(device)\n",
        "\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "gH7J7-DP841h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    return model, optimizer#, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "\n",
        "    f_path = checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "metadata": {
        "id": "5aNMHDiTKUk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm.notebook as tq\n",
        "from sklearn.metrics import jaccard_score\n",
        "def train_model(training_loader, model, optimizer):\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader),\n",
        "                      leave=True, colour='steelblue')\n",
        "\n",
        "    count = 0\n",
        "    accuracy = 0\n",
        "    for batch_idx, data in loop:\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        target_labels = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        # forward\n",
        "        outputs = model(ids, mask, token_type_ids) # (batch,predict)=(32,8)\n",
        "        loss = loss_fn(outputs, target_labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # training accuracy\n",
        "        sigmoid_output = torch.sigmoid(outputs)\n",
        "        label_threshold = 0.5\n",
        "        predicted_labels = torch.where(sigmoid_output > label_threshold, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "        count+=1\n",
        "        accuracy += jaccard_score(target_labels.cpu(), predicted_labels.cpu(), average='samples')\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        # grad descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_description(f\"\")\n",
        "        loop.set_postfix(batch_loss=loss)\n",
        "\n",
        "    # returning: trained model, model accuracy, mean loss\n",
        "    return model, float(accuracy)/count, np.mean(losses)"
      ],
      "metadata": {
        "id": "fq4IcOo786dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(validation_loader, model, optimizer):\n",
        "    losses = []\n",
        "    # set model to eval mode (turn off dropout, fix batch norm)\n",
        "    model.eval()\n",
        "\n",
        "    count = 0\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            target_labels = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, target_labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # validation accuracy\n",
        "            sigmoid_output = torch.sigmoid(outputs)\n",
        "            label_threshold = 0.5\n",
        "            predicted_labels = torch.where(sigmoid_output > label_threshold, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "            count+=1\n",
        "            accuracy += jaccard_score(target_labels.cpu(), predicted_labels.cpu(), average='samples')\n",
        "\n",
        "    return float(accuracy)/count, np.mean(losses)"
      ],
      "metadata": {
        "id": "Q26G26al8-ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for epoch in range(1, EPOCHS+1):\n",
        "#     print(f'Epoch {epoch}/{EPOCHS}')\n",
        "#     model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
        "#     val_acc, val_loss = eval_model(val_data_loader, model, optimizer)\n",
        "\n",
        "#     print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f} train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "-i5bP5GYGvA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, data_loader):\n",
        "    model = model.eval()\n",
        "\n",
        "    quotes = []\n",
        "    predictions = []\n",
        "    prediction_probs = []\n",
        "    target_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data in data_loader:\n",
        "        # quote = data[\"quote\"]\n",
        "        ids = data[\"input_ids\"].to(device, dtype = torch.long)\n",
        "        mask = data[\"attention_mask\"].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        target_labels = data[\"targets\"].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        sigmoid_output = torch.sigmoid(outputs)\n",
        "        label_threshold = 0.5\n",
        "        predicted_labels = torch.where(sigmoid_output > label_threshold, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "        # quotes.extend(quote)\n",
        "        predictions.extend(predicted_labels)\n",
        "        prediction_probs.extend(sigmoid_output)\n",
        "        target_values.extend(target_labels)\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu()\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "    target_values = torch.stack(target_values).cpu()\n",
        "\n",
        "    return quotes, predictions, prediction_probs, target_values\n"
      ],
      "metadata": {
        "id": "DvVdmUuwWji_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quotes, predictions, prediction_probs, target_values = get_predictions(model, test_data_loader)"
      ],
      "metadata": {
        "id": "9PjMbrHtp3P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(classification_report(target_values, predictions, target_names=tags_to_keep))"
      ],
      "metadata": {
        "id": "hzEWPcUiqTFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "def show_confusion_matrix(confusion_matrix):\n",
        "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "    plt.ylabel('True category')\n",
        "    plt.xlabel('Predicted category');"
      ],
      "metadata": {
        "id": "fSdWil22qkMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.rcParams[\"figure.figsize\"] = (10,7)\n",
        "# # cm = confusion_matrix(target_values, predictions)\n",
        "# cm = confusion_matrix(torch.argmax(target_values, dim=1), torch.argmax(predictions, dim=1))\n",
        "# df_cm = pd.DataFrame(cm, index=tags_to_keep, columns=tags_to_keep)\n",
        "# show_confusion_matrix(df_cm)"
      ],
      "metadata": {
        "id": "TLwriE4drhmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'drive/MyDrive/NLP_Final_Project/checkpoints/'\n",
        "\n",
        "seeds = [204, 9, 17]\n",
        "metrics_df = pd.DataFrame(columns=['Seed', 'Epoch', 'Training_Accuracy', 'Training_Loss', 'Validation_Accuracy', 'Validation_Loss'])\n",
        "\n",
        "test_metrics_df = pd.DataFrame(columns=['Seed', 'Test_Accuracy', 'Test_Loss'])\n",
        "\n",
        "for seed in seeds:\n",
        "\n",
        "    print(f\"----------Seed {seed}----------\")\n",
        "\n",
        "    train_df, valid_df, test_df = split_data(dataset, seed)\n",
        "\n",
        "    print('Training dataset label proportion:')\n",
        "    print(train_df[tags_to_keep].mean())\n",
        "    print('\\nTesting dataset label proportion: ')\n",
        "    print(test_df[tags_to_keep].mean())\n",
        "    print('\\nValidation dataset label proportion: ')\n",
        "    print(valid_df[tags_to_keep].mean())\n",
        "\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "    valid_dataset = CustomDataset(valid_df, tokenizer, MAX_LEN)\n",
        "    test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=TRAIN_BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    val_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=VALID_BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    test_data_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=VALID_BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    model = DistilBERTClass()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        print(f'Seed - {seed} , Epoch {epoch}/{EPOCHS}')\n",
        "        model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
        "        val_acc, val_loss = eval_model(val_data_loader, model, optimizer)\n",
        "\n",
        "        print(f'Train Accuracy={train_acc:.4f}, Train Loss={train_loss:.4f}, Validation Accuracy={val_acc:.4f}, Validation Loss={val_loss:.4f}')\n",
        "\n",
        "        epoch_data = {\n",
        "            'Seed': seed,\n",
        "            'Epoch': epoch,\n",
        "            'Training_Accuracy': train_acc,\n",
        "            'Training_Loss': train_loss,\n",
        "            'Validation_Accuracy': val_acc,\n",
        "            'Validation_Loss': val_loss\n",
        "        }\n",
        "\n",
        "        metrics_df.loc[len(metrics_df.index)] = epoch_data\n",
        "\n",
        "    test_acc, test_loss = eval_model(test_data_loader, model, optimizer)\n",
        "\n",
        "    test_metrics_data = {\n",
        "            'Seed': seed,\n",
        "            'Test_Accuracy': test_acc,\n",
        "            'Test_Loss': test_loss,\n",
        "        }\n",
        "    test_metrics_df.loc[len(test_metrics_df.index)] = test_metrics_data\n",
        "\n",
        "    print(f'Test Accuracy={test_acc:.4f}, Test Loss={test_loss:.4f}')\n",
        "\n",
        "    checkpoint = {\n",
        "        'test_accuracy ': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "\n",
        "    save_ckp(checkpoint, False, checkpoint_path + f'distilBert-{seed}.pt', '')\n",
        "\n",
        "    quotes, predictions, prediction_probs, target_values = get_predictions(model, test_data_loader)\n",
        "\n",
        "    with open(checkpoint_path+f'distilBert-{seed}-prediction-data.pkl', 'wb') as f:\n",
        "        pickle.dump((predictions, prediction_probs, target_values,metrics_df , test_metrics_df), f)\n",
        ""
      ],
      "metadata": {
        "id": "xKH2p-hf4Csf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------Seed 204----------\n",
        "# {'inspirational': 8147, 'humor': 4613, 'life': 7695, 'philosophy': 6535, 'love': 7320}\n",
        "# {'inspirational': 1437, 'philosophy': 1155, 'love': 1293, 'humor': 814, 'life': 1359}\n",
        "# {'life': 2266, 'humor': 1357, 'love': 2154, 'inspirational': 2396, 'philosophy': 1923}\n",
        "# Seed - 204 , Epoch 1/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  1.99it/s, batch_loss=tensor(0.4994, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.4829, Train Loss=0.4829, Validation Accuracy=0.5588, Validation Loss=0.4374\n",
        "# Seed - 204 , Epoch 2/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  1.98it/s, batch_loss=tensor(0.4973, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.6131, Train Loss=0.6131, Validation Accuracy=0.5966, Validation Loss=0.4228\n",
        "# Seed - 204 , Epoch 3/10\n",
        "# 100%\n",
        "#  638/638 [07:03<00:00,  1.99it/s, batch_loss=tensor(0.5584, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.6871, Train Loss=0.6871, Validation Accuracy=0.6161, Validation Loss=0.4195\n",
        "# Seed - 204 , Epoch 4/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.00it/s, batch_loss=tensor(0.3606, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.7547, Train Loss=0.7547, Validation Accuracy=0.6202, Validation Loss=0.4390\n",
        "# Seed - 204 , Epoch 5/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  1.97it/s, batch_loss=tensor(0.1249, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.8113, Train Loss=0.8113, Validation Accuracy=0.6281, Validation Loss=0.4694\n",
        "# Seed - 204 , Epoch 6/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  2.02it/s, batch_loss=tensor(0.0941, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.8619, Train Loss=0.8619, Validation Accuracy=0.6290, Validation Loss=0.5133\n",
        "# Seed - 204 , Epoch 7/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.00it/s, batch_loss=tensor(0.0354, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.8985, Train Loss=0.8985, Validation Accuracy=0.6371, Validation Loss=0.5421\n",
        "# Seed - 204 , Epoch 8/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.01it/s, batch_loss=tensor(0.0161, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9251, Train Loss=0.9251, Validation Accuracy=0.6283, Validation Loss=0.6104\n",
        "# Seed - 204 , Epoch 9/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.00it/s, batch_loss=tensor(0.1120, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9439, Train Loss=0.9439, Validation Accuracy=0.6309, Validation Loss=0.6822\n",
        "# Seed - 204 , Epoch 10/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.00it/s, batch_loss=tensor(0.1564, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9584, Train Loss=0.9584, Validation Accuracy=0.6316, Validation Loss=0.7278\n",
        "# Test Accuracy=0.6199, Test Loss=0.7444\n",
        "# ----------Seed 9----------\n",
        "# {'inspirational': 8147, 'love': 7320, 'life': 7695, 'philosophy': 6535, 'humor': 4613}\n",
        "# {'love': 1293, 'life': 1359, 'inspirational': 1437, 'philosophy': 1155, 'humor': 814}\n",
        "# {'love': 2154, 'life': 2266, 'inspirational': 2396, 'philosophy': 1923, 'humor': 1357}\n",
        "# Seed - 9 , Epoch 1/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  1.98it/s, batch_loss=tensor(0.3955, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.4821, Train Loss=0.4821, Validation Accuracy=0.5599, Validation Loss=0.4446\n",
        "# Seed - 9 , Epoch 2/10\n",
        "# 100%\n",
        "#  638/638 [07:03<00:00,  1.95it/s, batch_loss=tensor(0.4635, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.6112, Train Loss=0.6112, Validation Accuracy=0.5851, Validation Loss=0.4273\n",
        "# Seed - 9 , Epoch 3/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  1.97it/s, batch_loss=tensor(0.3743, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.6819, Train Loss=0.6819, Validation Accuracy=0.5962, Validation Loss=0.4280\n",
        "# Seed - 9 , Epoch 4/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  1.96it/s, batch_loss=tensor(0.1169, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.7524, Train Loss=0.7524, Validation Accuracy=0.6065, Validation Loss=0.4632\n",
        "# Seed - 9 , Epoch 5/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.01it/s, batch_loss=tensor(0.2218, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.8127, Train Loss=0.8127, Validation Accuracy=0.6131, Validation Loss=0.4757\n",
        "# Seed - 9 , Epoch 6/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.02it/s, batch_loss=tensor(0.2330, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.8637, Train Loss=0.8637, Validation Accuracy=0.6097, Validation Loss=0.5281\n",
        "# Seed - 9 , Epoch 7/10\n",
        "# 100%\n",
        "#  638/638 [07:02<00:00,  2.00it/s, batch_loss=tensor(0.0455, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9015, Train Loss=0.9015, Validation Accuracy=0.6135, Validation Loss=0.6088\n",
        "# Seed - 9 , Epoch 8/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.01it/s, batch_loss=tensor(0.0427, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9261, Train Loss=0.9261, Validation Accuracy=0.6088, Validation Loss=0.6467\n",
        "# Seed - 9 , Epoch 9/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.01it/s, batch_loss=tensor(0.0820, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9453, Train Loss=0.9453, Validation Accuracy=0.6042, Validation Loss=0.7117\n",
        "# Seed - 9 , Epoch 10/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  1.95it/s, batch_loss=tensor(0.0501, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9579, Train Loss=0.9579, Validation Accuracy=0.6088, Validation Loss=0.7929\n",
        "# Test Accuracy=0.6178, Test Loss=0.7745\n",
        "# ----------Seed 17----------\n",
        "# {'humor': 4613, 'love': 7320, 'inspirational': 8147, 'philosophy': 6535, 'life': 7695}\n",
        "# {'inspirational': 1437, 'philosophy': 1155, 'humor': 814, 'love': 1293, 'life': 1359}\n",
        "# {'love': 2154, 'life': 2266, 'inspirational': 2396, 'philosophy': 1923, 'humor': 1357}\n",
        "# Seed - 17 , Epoch 1/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.01it/s, batch_loss=tensor(0.3524, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.4768, Train Loss=0.4768, Validation Accuracy=0.5604, Validation Loss=0.4357\n",
        "# Seed - 17 , Epoch 2/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  2.01it/s, batch_loss=tensor(0.2469, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.6103, Train Loss=0.6103, Validation Accuracy=0.5964, Validation Loss=0.4197\n",
        "# Seed - 17 , Epoch 3/10\n",
        "# 100%\n",
        "#  638/638 [07:00<00:00,  2.00it/s, batch_loss=tensor(0.2025, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.6820, Train Loss=0.6820, Validation Accuracy=0.6132, Validation Loss=0.4151\n",
        "# Seed - 17 , Epoch 4/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  2.00it/s, batch_loss=tensor(0.0879, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.7483, Train Loss=0.7483, Validation Accuracy=0.6135, Validation Loss=0.4505\n",
        "# Seed - 17 , Epoch 5/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  2.00it/s, batch_loss=tensor(0.0710, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.8082, Train Loss=0.8082, Validation Accuracy=0.6149, Validation Loss=0.4744\n",
        "# Seed - 17 , Epoch 6/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  1.99it/s, batch_loss=tensor(0.4595, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.8570, Train Loss=0.8570, Validation Accuracy=0.6293, Validation Loss=0.5137\n",
        "# Seed - 17 , Epoch 7/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  2.01it/s, batch_loss=tensor(0.0308, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.8948, Train Loss=0.8948, Validation Accuracy=0.6280, Validation Loss=0.5693\n",
        "# Seed - 17 , Epoch 8/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  1.95it/s, batch_loss=tensor(0.1092, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9233, Train Loss=0.9233, Validation Accuracy=0.6225, Validation Loss=0.6264\n",
        "# Seed - 17 , Epoch 9/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  1.95it/s, batch_loss=tensor(0.1136, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9409, Train Loss=0.9409, Validation Accuracy=0.6281, Validation Loss=0.6690\n",
        "# Seed - 17 , Epoch 10/10\n",
        "# 100%\n",
        "#  638/638 [07:01<00:00,  1.97it/s, batch_loss=tensor(0.0475, device='cuda:0',        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)]\n",
        "# Train Accuracy=0.9528, Train Loss=0.9528, Validation Accuracy=0.6218, Validation Loss=0.7199\n",
        "# Test Accuracy=0.6309, Test Loss=0.7132"
      ],
      "metadata": {
        "id": "PA875e9u1V3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_Z5WNuprVcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}