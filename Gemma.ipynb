{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JehaNXKfQmFY"
      },
      "outputs": [],
      "source": [
        "# !pip install bitsandbytes accelerate\n",
        "# !pip install sentence_transformers\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "import pickle\n",
        "import tqdm.notebook as tq\n",
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LJpdQDn_T5be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "pP61fu6lWc99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpretation_dataset = pd.read_excel(\"drive/MyDrive/NLP_Final_Project/dataset/final_interpretations.xlsx\")\n",
        "interpretation_dataset['quote_len'] = interpretation_dataset['Quote'].apply(len)\n",
        "interpretation_dataset = interpretation_dataset[interpretation_dataset['quote_len']<256]\n",
        "\n",
        "print('Dataset shape:',interpretation_dataset.shape)"
      ],
      "metadata": {
        "id": "TxFdh3gxWsKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google/gemma-2b'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "sentence_model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "qKOoJjCZXHXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = '''Explain the meaning of the quote - \\n\n",
        "# \\n\n",
        "# Quote : \"a smile is what makes a face beautiful.\"\\n\n",
        "# Answer : \"This quote implies that a smile enhances one's facial attractiveness more than any other physical feature, suggesting that genuine joy and warmth are key components of beauty.\"\\n\n",
        "# \\n\n",
        "# Quote : \"sometimes there a hundred lies behind a smile and not a single truth behind a tear.\"\\n\n",
        "# Answer :  \"This quote suggests that smiles can often be used to hide true feelings or dishonest intentions, portraying a facade of happiness or contentment. In contrast, tears are depicted as more genuine expressions, less likely to be used to conceal the truth, indicating raw, unfiltered emotion.\"\\n\n",
        "# \\n\n",
        "# Quote : \"peace begins with a smile\"\\n\n",
        "# Answer : '''\n",
        "\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "# outputs = model.generate(**inputs, max_new_tokens = 512, do_sample=True, temperature=0.5, top_p=0.95, repetition_penalty=1.6)\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "JZtj2M7kUX25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quote_string = \"Quote\"\n",
        "tag = \"Tag\"\n",
        "meaning = \"Interpretation\"\n",
        "new_inter = \"Generated_Interpretation\"\n",
        "\n",
        "class QuoteInterpreter():\n",
        "\n",
        "    def __init__(self, model, tokenizer,  sentence_model) -> None:\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model.to(self.device)\n",
        "        self.sentence_model = sentence_model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.sentence_model.eval()\n",
        "\n",
        "    def generate_embeddings(self, data):\n",
        "        embeds = {}\n",
        "        for row in data.itertuples(index=True):\n",
        "            idx = row.Index\n",
        "            with torch.no_grad():\n",
        "                embedding = self.sentence_model.encode(getattr(row, quote_string), convert_to_tensor=True).to(self.device)\n",
        "            embeds[idx] = embedding\n",
        "        return embeds\n",
        "\n",
        "    def prompt(self, data, method=\"zero_shot\", n = 0):\n",
        "        if method == \"few_shot\":\n",
        "            self.embeddings = self.generate_embeddings(data)\n",
        "\n",
        "        loop = tq.tqdm(data.itertuples(index=True), total=len(data),\n",
        "                      leave=True, colour='steelblue')\n",
        "\n",
        "        for row in loop:\n",
        "            i = row.Index\n",
        "            prompt = self.get_prompt(i, data, method, n)\n",
        "            data.loc[i, method] = prompt\n",
        "            with torch.no_grad():\n",
        "                input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "                outputs = self.model.generate(\n",
        "                    input_ids,\n",
        "                    min_new_tokens=20,\n",
        "                    num_beams=5,\n",
        "                    max_new_tokens = 250,\n",
        "                    num_return_sequences=3,\n",
        "                    no_repeat_ngram_size=2)\n",
        "            for idx, beam_output in enumerate(outputs):\n",
        "                sent = self.tokenizer.decode(beam_output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                data.loc[i, new_inter +'_'+ str(idx+1)] = sent\n",
        "\n",
        "            loop.set_description(f\"\")\n",
        "        return data\n",
        "\n",
        "    def get_prompt(self, quote_idx, data, method, n):\n",
        "        if method == \"cot\":\n",
        "            return self.cot_prompt(data.loc[quote_idx, quote_string], data.loc[quote_idx, tag])\n",
        "\n",
        "        if method == \"few_shot\":\n",
        "            example_idx = self.get_examples(quote_idx, data, n)\n",
        "            return self.few_shot_prompt(data.loc[quote_idx, quote_string], data, examples=example_idx)\n",
        "\n",
        "        return self.zero_shot_prompt(data.loc[quote_idx, quote_string])\n",
        "\n",
        "    def zero_shot_prompt(self, quote):\n",
        "        return f\"Explain the meaning of the quote - \\n  Quote : '{quote}'\\n Answer : \"\n",
        "\n",
        "    def get_examples(self, quote_idx, df, n):\n",
        "        quote_embed = self.embeddings[quote_idx]\n",
        "        similar_quotes = df[df[tag] == df.loc[quote_idx, tag]]\n",
        "        similar_quotes = similar_quotes.drop(quote_idx)\n",
        "        similar_quotes = [(row.Index, self.embeddings[row.Index]) for row in similar_quotes.itertuples()]\n",
        "        cos_sims = {}\n",
        "        for i in similar_quotes:\n",
        "            cos_sims[i[0]] = float(util.cos_sim(quote_embed, i[1])[0][0].cpu())\n",
        "        top_n = sorted(cos_sims.items(), key=lambda x: x[1], reverse=True)[:n]\n",
        "        return [i[0] for i in top_n]\n",
        "\n",
        "    def few_shot_prompt(self, quote, df, examples=None):\n",
        "        ext_prompt = \"\"\n",
        "        if examples is not None:\n",
        "            for i in examples:\n",
        "                e_quote, e_meaning = df.loc[i, quote_string], df.loc[i, meaning]\n",
        "                ext_prompt += f\"Quote: '{e_quote}'\\n Answer : {e_meaning}\\n\\n\"\n",
        "\n",
        "        return f\"Explain the meaning of the quote - \\n  {ext_prompt} Quote : {quote}\\n Answer : \"\n"
      ],
      "metadata": {
        "id": "IvDY2zxAfTS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = QuoteInterpreter(model, tokenizer, sentence_model)"
      ],
      "metadata": {
        "id": "ios6DM13hNPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_df = interpreter.prompt(interpretation_dataset.copy(), method=\"zero_shot\", n = 0)\n",
        "\n",
        "import pickle\n",
        "\n",
        "checkpoint_path = 'drive/MyDrive/NLP_Final_Project/checkpoints/'\n",
        "# with open(checkpoint_path+f'Gemma-2b-Zero-Shot.pkl', 'wb') as f:\n",
        "#     pickle.dump((zero_shot_df), f)\n",
        "\n",
        "zero_shot_df.to_csv(checkpoint_path+'Gemma-2b-Zero-Shot.csv')"
      ],
      "metadata": {
        "id": "d5w_hi8jhSMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_shot_df = interpreter.prompt(interpretation_dataset.copy(), method=\"few_shot\", n = 1)\n",
        "\n",
        "checkpoint_path = 'drive/MyDrive/NLP_Final_Project/checkpoints/'\n",
        "# with open(checkpoint_path+f'Gemma-2b-Zero-Shot.pkl', 'wb') as f:\n",
        "#     pickle.dump((zero_shot_df), f)\n",
        "\n",
        "one_shot_df.to_csv(checkpoint_path+'Gemma-2b-One-Shot.csv')"
      ],
      "metadata": {
        "id": "OVaou2EsjQ55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_df = interpreter.prompt(interpretation_dataset.copy(), method=\"few_shot\", n = 3)\n",
        "\n",
        "checkpoint_path = 'drive/MyDrive/NLP_Final_Project/checkpoints/'\n",
        "# with open(checkpoint_path+f'Gemma-2b-Zero-Shot.pkl', 'wb') as f:\n",
        "#     pickle.dump((zero_shot_df), f)\n",
        "\n",
        "few_shot_df.to_csv(checkpoint_path+'Gemma-2b-Few-Shot.csv')"
      ],
      "metadata": {
        "id": "qb-Icaesh3Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HWO8Eb29WRuS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}